data(Vada_P20_meteo)
data(Cecina_P17_meteo)
data(Livorno_P20_meteo)
data(Pisa_P33_meteo)
data(Lido_Camaiore_P40_meteo)
data(Marinella_di_Sarzana_P46_meteo)
###
data(Livorno_P25_meteo)
data(Livorno_P25_meteo)
devtools::install_github("adamhsparks/GSODR
")
devtools::install_github("adamhsparks/GSODR")
library(GSODR)
devtools::install_github("environmentalinformatics-marburg/GSODTools")
shp_kibo <- GSODTools::stationFromCoords(x = 37.359031, y = -3.065053, width = 500)
library(GSODTools)
devtools::install_github("environmentalinformatics-marburg/GSODTools")
library(GSODTools)
shp_kibo <- stationFromCoords(x = 37.359031, y = -3.065053, width = 500)
library(timeDate)
install.packages("timedate")
install.packages("timeDate")
install.packages("timeDate")
library(GSODTools)
gsodstations
firenze <- filter(gsodstations, STATION.NAME == "FIRENZE")
firenze
firenze <- filter(gsodstations, STATION.NAME == "PERETOLA")
firenze
firenze <- filter(gsodstations, STATION.NAME == "PISA")
firenze
firenze <- filter(gsodstations, STATION.NAME == "SANGIUSTO")
firenze
head(gsodstations)
gsodstations$CTRY
which(gsodstations$CTRY =="IT")
which(gsodstations$CTRY =="ITA")
levels(gsodstations$CTRY)
which(gsodstations$CTRY =="IY")
which(gsodstations$CTRY =="IT")
levels(gsodstations$CTRY)
gsodstations
firenze <- filter(gsodstations, STATION.NAME == "FIRENZE")
firenze
library(weatherData)
palermo_code=getStationCode("Cochin")
palermo_code
palermo_code=getStationCode("Catania")
palermo_code
palermo_code=getStationCode("Palermo")
Sys.Date()
data_available_today=showAvailableColumns("LICJ", Sys.Date(), opt_detailed=T)
data_available_today
data_available_today_CT=showAvailableColumns("LICC", Sys.Date(), opt_detailed=T)
getDetailedWeather("LICJ", Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T,station_type="id")
Giarre_data_today
Barcellona_data_today=getDetailedWeather("IMESSINA4",Sys.Date(), opt_all_columns=T,station_type="id")
Barcellona_data_today
library(weatherData)
data(IntlWxStations)
IntlWxStations
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
for ( i in seq_along(not_retweet_with_authors))
{
rank_message_retweet$retweeted_authors[id_na_message_retweet[i]]=as.character(channel_obj$screeName[min(which(channel_obj$text ==not_retweet_with_authors[i]))])
}
rank_message_retweet$data=NA
for ( i in seq_along(rank_message_retweet$message))
{
rank_message_retweet$data[i]=as.character(channel_obj$data[min(which((channel_obj$text %in%  rank_message_retweet$message[i] )==T))])
}
ls_hash=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_hash(x,extract=T))
ls_tag=lapply(channel_obj$text,FUN=function(x) extract_mentions(x))
ls_links=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_url(x, extract=TRUE))
ls_lenhash=unlist(lapply(ls_hash,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_hash(x,extract=T)[[1]]))))
ls_lenlinks=unlist(lapply( ls_links,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_url(x, extract=TRUE)[[1]]))))
ls_lentag=unlist(lapply(ls_tag,FUN=function(x) ifelse(is.na(x),0, length(extract_mentions(x)[[1]]))))
ls_words=unlist(lapply(channel_obj$text,FUN=function(x) qdap::word_count(x)))
####################################################################################
# Extract replies and organize a frame
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
install.packages("Rselenium")
install.packages("RSelenium")
library(RSelenium)
download_channel=function(channel,outfile,format,start_date,end_date,user,pass) {
url_web=paste0("http://disit.org/tv/query/query.php?channel=",channel,"&start_date=",start_date,"&end_date=",end_date,"&format=",format)
extra_web=paste0("--user ",user," --password ",pass)
download.file(url_web,destfile=outfile,method="wget",extra = extra_web)
}
mese=c("03","04","05","06","07","08","09","10","11")
i=5
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=6
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=7
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=8
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile="CALDO_A.csv",
format="csv",
start_date="2015-03-01",
end_date="2015-05-01",
user="crisci",
pass="v5TTdsug")
mese=c("03","04","05","06","07","08","09","10","11")
for ( i in 3:8){
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],".csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
}
i=5
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=6
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=7
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=8
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.plugin.sentiment)
install.packages("tm.plugin.sentiment")
install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) # posted comments on SO about this not working
library(tm)
Here is a starting point. First is some code to install the sentiment plug ins (thank you, Dason, for the useful comment).
Next, with some text from a previous SO post to show what you might do, you can create a data frame.
Install the packages for sentiment analysis:
# install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) # posted comments on SO about this not working
library(tm)
Using the installed functions:
some_txt<- c("I am very happy at stack overflow , excited, and optimistic.",
"I am very scared from OP question, annoyed, and irritated.", "I am completely neutral about blandness.")
corpus <- Corpus(VectorSource(some_txt))
pos <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Positiv")))
neg <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Negativ")))
pos.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE)),
terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below
neg.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE)),
terms_in_General_Inquirer_categories("Negativ"))
pos.score
?terms_in_General_Inquirer_categories
library(raster)
library(ncdf)
library(rts)
setwd("/home/alf/Scrivania/lav_caldo_tw")
dir()
library(raster)
library(ncdf)
library(rts)
library(leaflet)
setwd("/home/alf/Scrivania/lav_caldo_tw")
source("auxillary_functions.r")
ita_bound=getData('GADM', country='ITA', level=0)
library(weatherData)
ita_bound
plot(ita_bound)
tmax_italia=brick("redlav_tmax.nc",varname=c("tmax"))
tmax_italia
source("auxillary_functions.r")
tmax_italia=brick("redlav_tmax.nc",varname=c("tmax"))
tmed_italia=brick("redlav_tmed.nc",varname=c("tmed"))
tmin_italia=brick("redlav_min.nc",varname=c("tmin"))
urel_italia=brick("redlav_urel.nc",varname=c("urel"))
prec_italia=brick("redlav_prec.nc",varname=c("prec"))
dates=as.Date(as.POSIXct(tmax_italia@z$hours*3600, origin="1992-01-01"))
caldo_data=read.csv("caldo_data.csv")
index_date_06=grep("-06-",caldo_data$date)
index_date_07=grep("-07-",caldo_data$date)
index_date_08=grep("-08-",caldo_data$date)
index_date_09=grep("-09-",caldo_data$date)
index_date_r_06=grep("-06-",dates)
index_date_r_07=grep("-07-",dates)
index_date_r_08=grep("-08-",dates)
index_date_r_09=grep("-09-",dates)
tmax_italia06=stack(tmax_italia[[index_date_r_06]])
caldo_native_06=caldo_data$native[index_date_06]
tmax_italia06_out=tmax_italia06[[1]]*NA
names(tmax_italia06_out)="sign_06_tmax"
tmax_italia06_out=setValues(tmax_italia06_out,as.numeric(apply(as.data.frame(tmax_italia06),1,function(x) { summary(lm(x ~ caldo_native_06))$coefficients[8] })))
tmax_italia06_out_s=reclassify(tmax_italia06_out, c(-Inf,0.05,2, 0.051,0.10,1,0.101,Inf,NA))
tmax_italia06_s=crop_raster(tmax_italia06_out_s,ita_bound)
tmax_italia06_s=crop_raster(ita_bound,tmax_italia06_out_s)
tmax_italia06_r=crop_raster(ita_bound,tmax_italia06_out)
plot(tmax_italia06_r)
plot(tmax_italia06_s)
tmax_italia06_out_s=reclassify(tmax_italia06_out, c(-Inf,0.05,1, 0.0505,Inf,NA))
tmax_italia06_s=crop_raster(ita_bound,tmax_italia06_out_s)
plot(tmax_italia06_s)
saveRDS(tmax_italia06_r,"tmax_italia06_r.rds")
plot(tmax_italia06_r)
tmax_italia07=stack(tmax_italia[[index_date_r_07]])
caldo_native_07=caldo_data$native[index_date_07]
tmax_italia07_out=tmax_italia07[[1]]*NA
names(tmax_italia07_out)="sign_07_tmax"
tmax_italia07_out=setValues(tmax_italia07_out,as.numeric(apply(as.data.frame(tmax_italia07)[,1:15],1,function(x) { summary(lm(x[1:15] ~ caldo_native_07))$coefficients[8] })))
tmax_italia07_out_s=reclassify(tmax_italia07_out, c(-Inf,0.05,2, 0.051,0.10,1,0.101,Inf,NA))
tmax_italia07_s=crop_raster(ita_bound,tmax_italia07_out_s)
tmax_italia07_r=crop_raster(ita_bound,tmax_italia07_out)
saveRDS(tmax_italia07_s,"tmax_italia06_s.rds")
saveRDS(tmax_italia07_r,"tmax_italia06_r.rds")
names(tmax_italia07_out)="sign_07_tmax"
tmax_italia07_out=setValues(tmax_italia07_out,as.numeric(apply(as.data.frame(tmax_italia07),1,function(x) { summary(lm(x[1:15] ~ caldo_native_07))$coefficients[8] })))
tmax_italia07_out_s=reclassify(tmax_italia07_out, c(-Inf,0.05,2, 0.051,0.10,1,0.101,Inf,NA))
tmax_italia07=stack(tmax_italia[[index_date_r_07]])
caldo_native_07=caldo_data$native[index_date_07]
tmax_italia07_out=tmax_italia07[[1]]*NA
names(tmax_italia07_out)="sign_07_tmax"
tmax_italia07_out=setValues(tmax_italia07_out,as.numeric(apply(as.data.frame(tmax_italia07),1,function(x) { summary(lm(x ~ caldo_native_07))$coefficients[8] })))
tmax_italia07_out_s=reclassify(tmax_italia07_out, c(-Inf,0.05,2, 0.051,0.10,1,0.101,Inf,NA))
tmax_italia07=stack(tmax_italia[[index_date_r_07]])
caldo_native_07=caldo_data$native[index_date_07]
tmax_italia07_out=tmax_italia07[[1]]*NA
names(tmax_italia07_out)="sign_07_tmax"
tmax_italia07_out=setValues(tmax_italia07_out,as.numeric(apply(as.data.frame(tmax_italia07),1,function(x) { summary(lm(x ~ caldo_native_07))$coefficients[8] })))
tmax_italia07_out_s=reclassify(tmax_italia07_out, c(-Inf,0.05,2, 0.051,0.10,1,0.101,Inf,NA))
tmax_italia07_s=crop_raster(ita_bound,tmax_italia07_out_s)
tmax_italia07_r=crop_raster(ita_bound,tmax_italia07_out)
tmax_italia07_s=crop_raster(ita_bound,tmax_italia07_out_s)
tmax_italia07_r=crop_raster(ita_bound,tmax_italia07_out)
plot(tmax_italia07_s)
saveRDS(tmax_italia07_r,"tmax_italia06_r.rds")
tmax_italia07_out_s=reclassify(tmax_italia07_out, c(-Inf,0.05,1, 0.0505,Inf,NA))
tmax_italia07_s=crop_raster(ita_bound,tmax_italia07_out_s)
tmax_italia07_r=crop_raster(ita_bound,tmax_italia07_out)
saveRDS(tmax_italia07_s,"tmax_italia06_s.rds")
saveRDS(tmax_italia07_r,"tmax_italia06_r.rds")
plot(tmax_italia07_s)
plot(tmax_italia07_r)
plot(tmax_italia07_s)
tmax_italia08=stack(tmax_italia[[index_date_r_08]])
caldo_native_08=caldo_data$native[index_date_08]
tmax_italia08_out=tmax_italia08[[1]]*NA
names(tmax_italia08_out)="sign_08_tmax"
tmax_italia08_out=setValues(tmax_italia08_out,as.numeric(apply(as.data.frame(tmax_italia08),1,function(x) { summary(lm(x ~ caldo_native_08))$coefficients[8] })))
tmax_italia08_out_s=reclassify(tmax_italia08_out, c(-Inf,0.05,1, 0.0505,Inf,NA))
tmax_italia08_s=crop_raster(ita_bound,tmax_italia08_out_s)
tmax_italia08_r=crop_raster(ita_bound,tmax_italia08_out)
saveRDS(tmax_italia07_s,"tmax_italia07_s.rds")
saveRDS(tmax_italia07_r,"tmax_italia07_r.rds")
saveRDS(tmax_italia08_s,"tmax_italia08_s.rds")
saveRDS(tmax_italia08_r,"tmax_italia08_r.rds")
plot(tmax_italia08_s)
plot(tmax_italia08_r)
plot(tmax_italia08_s)
plot(tmax_italia08_r)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,46),col=brewer.pal(2,"Reds"),legend=F)
plot(nations_bounds,xlim = c(5, 20), ylim = c(35, 46),asp = 1,add=T)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(2,"Reds"),legend=F)
plot(nations_bounds,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
plot(ita_bounds,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(2,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(2,"Reds"), horiz=TRUE)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(3,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(3,"Reds"), horiz=TRUE)
png(file = "tmax_italia08_r.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(3,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=-.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(3,"Reds"), horiz=TRUE)
dev.off()
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(20,"YdOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(20,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(5,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
plot(tmax_italia08_r,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(5,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
png(file = "tmax_italia07_r.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia07_r,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(5,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
dev.off()
png(file = "tmax_italia07_s.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia07_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(3,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(3,"Reds"), horiz=TRUE)
dev.off()
png(file = "tmax_italia06_r.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia06_r,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(5,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
dev.off()
png(file = "tmax_italia06_s.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia06_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(3,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(3,"Reds"), horiz=TRUE)
dev.off()
png(file = "tmax_italia08_s.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia08_s,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(3,"Reds"),legend=F)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
legend("bottomleft", inset=.05, title="Social network Association",c("No","Yes"), fill=brewer.pal(3,"Reds"), horiz=TRUE)
dev.off()
png(file = "tmax_italia08_r.png",width = 1024, height = 768, bg = "transparent")
plot(tmax_italia08_r,xlim = c(-5, 20), ylim = c(35,49),col=brewer.pal(5,"YlOrRd"),legend=T)
plot(ita_bound,xlim = c(5, 20), ylim = c(35, 49),asp = 1,add=T)
dev.off()
